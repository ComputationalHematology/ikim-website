@Article{Rietberg2023_mdpi_classifying-ms-patient-reports,
  author         = {Rietberg, Max Tigo and Nguyen, Van Bach and Geerdink, Jeroen and Vijlbrief, Onno and Seifert, Christin},
  journal        = {Diagnostics},
  title          = {Accurate and Reliable Classification of Unstructured Reports on Their Diagnostic Goal Using BERT Models},
  year           = {2023},
  issn           = {2075-4418},
  number         = {7},
  volume         = {13},
  abstract       = {Understanding the diagnostic goal of medical reports is valuable information for understanding patient flows. This work focuses on extracting the reason for taking an MRI scan of Multiple Sclerosis (MS) patients using the attached free-form reports: Diagnosis, Progression or Monitoring. We investigate the performance of domain-dependent and general state-of-the-art language models and their alignment with domain expertise. To this end, eXplainable Artificial Intelligence (XAI) techniques are used to acquire insight into the inner workings of the model, which are verified on their trustworthiness. The verified XAI explanations are then compared with explanations from a domain expert, to indirectly determine the reliability of the model. BERTje, a Dutch Bidirectional Encoder Representations from Transformers (BERT) model, outperforms RobBERT and MedRoBERTa.nl in both accuracy and reliability. The latter model (MedRoBERTa.nl) is a domain-specific model, while BERTje is a generic model, showing that domain-specific models are not always superior. Our validation of BERTje in a small prospective study shows promising results for the potential uptake of the model in a practical setting.},
  article-number = {1251},
  doi            = {10.3390/diagnostics13071251},
  url            = {https://www.mdpi.com/2075-4418/13/7/1251},
}

@Article{Borys2023_ejr_xai-in-medical-saliency,
  author   = {Katarzyna Borys and Yasmin {Alyssa Schmitt} and Meike Nauta and Christin Seifert and Nicole Krämer and Christoph M. Friedrich and Felix Nensa},
  journal  = {European Journal of Radiology},
  title    = {Explainable AI in Medical Imaging: An overview for clinical practitioners – Saliency-based XAI approaches},
  year     = {2023},
  issn     = {0720-048X},
  pages    = {110787},
  abstract = {Since recent achievements of Artificial Intelligence (AI) have proven significant success and promising results throughout many fields of application during the last decade, AI has also become an essential part of medical research. The improving data availability, coupled with advances in high-performance computing and innovative algorithms, has increased AI's potential in various aspects. Because AI rapidly reshapes research and promotes the development of personalized clinical care, alongside its implementation arises an urgent need for a deep understanding of its inner workings, especially in high-stake domains. However, such systems can be highly complex and opaque, limiting the possibility of an immediate understanding of the system’s decisions. Regarding the medical field, a high impact is attributed to these decisions as physicians and patients can only fully trust AI systems when reasonably communicating the origin of their results, simultaneously enabling the identification of errors and biases. Explainable AI (XAI), becoming an increasingly important field of research in recent years, promotes the formulation of explainability methods and provides a rationale allowing users to comprehend the results generated by AI systems. In this paper, we investigate the application of XAI in medical imaging, addressing a broad audience, especially healthcare professionals. The content focuses on definitions and taxonomies, standard methods and approaches, advantages, limitations, and examples representing the current state of research regarding XAI in medical imaging. This paper focuses on saliency-based XAI methods, where the explanation can be provided directly on the input data (image) and which naturally are of special importance in medical imaging.},
  doi      = {https://doi.org/10.1016/j.ejrad.2023.110787},
  keywords = {Explainable AI, Medical Imaging, Radiology, Black-Box, Explainability, Interpretability},
  url      = {https://www.sciencedirect.com/science/article/pii/S0720048X23001018},
}

@Article{Borys2023_ejr_xai-in-medical-beyond-saliency,
  author   = {Katarzyna Borys and Yasmin Alyssa Schmitt and Meike Nauta and Christin Seifert and Nicole Krämer and Christoph M. Friedrich and Felix Nensa},
  journal  = {European Journal of Radiology},
  title    = {Explainable AI in medical imaging: An overview for clinical practitioners – Beyond saliency-based XAI approaches},
  year     = {2023},
  issn     = {0720-048X},
  pages    = {110786},
  volume   = {162},
  abstract = {Driven by recent advances in Artificial Intelligence (AI) and Computer Vision (CV), the implementation of AI systems in the medical domain increased correspondingly. This is especially true for the domain of medical imaging, in which the incorporation of AI aids several imaging-based tasks such as classification, segmentation, and registration. Moreover, AI reshapes medical research and contributes to the development of personalized clinical care. Consequently, alongside its extended implementation arises the need for an extensive understanding of AI systems and their inner workings, potentials, and limitations which the field of eXplainable AI (XAI) aims at. Because medical imaging is mainly associated with visual tasks, most explainability approaches incorporate saliency-based XAI methods. In contrast to that, in this article we would like to investigate the full potential of XAI methods in the field of medical imaging by specifically focusing on XAI techniques not relying on saliency, and providing diversified examples. We dedicate our investigation to a broad audience, but particularly healthcare professionals. Moreover, this work aims at establishing a common ground for cross-disciplinary understanding and exchange across disciplines between Deep Learning (DL) builders and healthcare professionals, which is why we aimed for a non-technical overview. Presented XAI methods are divided by a method’s output representation into the following categories: Case-based explanations, textual explanations, and auxiliary explanations.},
  doi      = {https://doi.org/10.1016/j.ejrad.2023.110786},
  keywords = {Explainable AI, Medical imaging, Radiology, Black-Box, Explainability, Interpretability},
  url      = {https://www.sciencedirect.com/science/article/pii/S0720048X23001006},
}


@misc{Haller2022_preprints_short-answare-grading-survey,
  title = {Survey on Automated Short Answer Grading with Deep Learning: From Word Embeddings to Transformers},
  author = {Haller, Stefan and Aldea, Adina and Seifert, Christin and Strisciuglio, Nicola},
  year = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2204.03503},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@article{Lu2023_tsnre_channel-contribution-sleep-scoring,
  author  = {Lu, Changqing and Pathak, Shreyasi and Englebienne, Gwenn and Seifert, Christin},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  title   = {Channel Contribution In Deep Learning Based Automatic Sleep Scoring – How Many Channels Do We Need?},
  year    = {2023},
  pages   = {494-505},
  doi     = {10.1109/TNSRE.2022.3227040},
}


@article{imangaliyevDiagnosisInflammatoryBowel2022,
  title = {Diagnosis of {{Inflammatory Bowel Disease}} and {{Colorectal Cancer}} through {{Multi-View Stacked Generalization Applied}} on {{Gut Microbiome Data}}},
  author = {Imangaliyev, Sultan and Schl{\"o}tterer, J{\"o}rg and Meyer, Folker and Seifert, Christin},
  year = {2022},
  month = oct,
  journal = {Diagnostics},
  volume = {12},
  number = {10},
  pages = {2514},
  issn = {2075-4418},
  doi = {10.3390/diagnostics12102514},
  abstract = {Most of the microbiome studies suggest that using ensemble models such as Random Forest results in best predictive power. In this study, we empirically evaluate a more powerful ensemble learning algorithm, multi-view stacked generalization, on pediatric inflammatory bowel disease and adult colorectal cancer patients' cohorts. We aim to check whether stacking would lead to better results compared to using a single best machine learning algorithm. Stacking achieves the best test set Average Precision (AP) on inflammatory bowel disease dataset reaching AP = 0.69, outperforming both the best base classifier (AP = 0.61) and the baseline meta learner built on top of base classifiers (AP = 0.63). On colorectal cancer dataset, the stacked classifier also outperforms (AP = 0.81) both the best base classifier (AP = 0.79) and the baseline meta learner (AP = 0.75). Stacking achieves best predictive performance on test set outperforming the best classifiers on both patient cohorts. Application of the stacking solves the issue of choosing the most appropriate machine learning algorithm by automating the model selection procedure. Clinical application of such a model is not limited to diagnosis task only, but it also can be extended to biomarker selection thanks to feature selection procedure.},
  langid = {english}
}

@article{keulenScalableInterdisciplinaryData2020,
  title = {Scalable {{Interdisciplinary Data Science Teaching}} at the {{University}} of {{Twente}}},
  author = {van Keulen, Maurice and Seifert, Christin and Poel, Mannes and {Groothuis-Oudshoorn}, Karin},
  year = {2020},
  month = dec,
  journal = {Berlin Journal of Data Science},
  volume = {1},
  issn = {978-3-00-064614-0},
  abstract = {Data scientists are in high demand in many disciplines and domains. This paper describes the data science course open to all master students of the University of Twente. We outline the main challenges of teaching a large and heterogeneous population of non-computer science students about data science and how we addressed them, as well as a historical perspective on how the course grew and evolved.},
  langid = {english}
}

@inproceedings{legoyRcATTRetrievingATT2020,
  title = {{{rcATT}}: Retrieving {{ATT}}\&{{CK}} Tactics and Techniques in Cyber Threat Reports},
  booktitle = {{{FIRST Cyber Threat Intelligence Symposium}}},
  author = {Legoy, Valentine and Caselli, Marco and Peter, Andreas and Seifert, Christin},
  year = {2020},
  month = mar,
  address = {{Zurich}}
}

@article{libbiGeneratingSyntheticTraining2021,
  title = {Generating {{Synthetic Training Data}} for {{Supervised De-Identification}} of {{Electronic Health Records}}},
  author = {Libbi, Claudia Alessandra and Trienes, Jan and Trieschnigg, Dolf and Seifert, Christin},
  year = {2021},
  journal = {Future Internet},
  volume = {13},
  number = {5},
  issn = {1999-5903},
  doi = {10.3390/fi13050136},
  abstract = {A major hurdle in the development of natural language processing (NLP) methods for Electronic Health Records (EHRs) is the lack of large, annotated datasets. Privacy concerns prevent the distribution of EHRs, and the annotation of data is known to be costly and cumbersome. Synthetic data presents a promising solution to the privacy concern, if synthetic data has comparable utility to real data and if it preserves the privacy of patients. However, the generation of synthetic text alone is not useful for NLP because of the lack of annotations. In this work, we propose the use of neural language models (LSTM and GPT-2) for generating artificial EHR text jointly with annotations for named-entity recognition. Our experiments show that artificial documents can be used to train a supervised named-entity recognition model for de-identification, which outperforms a state-of-the-art rule-based baseline. Moreover, we show that combining real data with synthetic data improves the recall of the method, without manual annotation effort. We conduct a user study to gain insights on the privacy of artificial text. We highlight privacy risks associated with language models to inform future research on privacy-preserving automated text generation and metrics for evaluating privacy-preservation during text generation.}
}

@incollection{liuRewritingFictionalTexts2021,
  title = {Rewriting {{Fictional Texts Using Pivot Paraphrase Generation}} and {{Character Modification}}},
  booktitle = {Text, {{Speech}}, and {{Dialogue}}},
  author = {Liu, Dou and Zhu, Tingting and Schl{\"o}tterer, J{\"o}rg and Seifert, Christin and Wang, Shenghui},
  year = {2021},
  pages = {73--85},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-030-83527-9_6}
}

@article{marazzaAutomaticProcessComparison2020,
  title = {Automatic {{Process Comparison}} for {{Subpopulations}}: {{Application}} in {{Cancer Care}}},
  author = {Marazza, Francesca and Bukhsh, Faiza Allah and Geerdink, Jeroen and Vijlbrief, Onno and Pathak, Shreyasi and van Keulen, Maurice and Seifert, Christin},
  year = {2020},
  journal = {Int. J. Environ. Res. Public Health},
  volume = {17},
  number = {16},
  doi = {10.3390/ijerph1716570}
}

@inproceedings{marinEffectivenessNeuralLanguage2020,
  title = {Effectiveness of Neural Language Models for Word Prediction of Textual Mammography Reports},
  booktitle = {Proc. {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}}},
  author = {Marin, Mihai David and Mocanu, Elena and Seifert, Christin},
  year = {2020},
  series = {{{IEEE SMC}}}
}

@Article{Nauta2023_csur_evaluating-xai-survey,
  author    = {Nauta, Meike and Trienes, Jan and Pathak, Shreyasi and Nguyen, Elisa and Peters, Michelle and Schmitt, Yasmin and Schl\"{o}tterer, J\"{o}rg and van Keulen, Maurice and Seifert, Christin},
  journal   = {ACM Comput. Surv.},
  title     = {From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI},
  year      = {2023},
  issn      = {0360-0300},
  month     = {feb},
  abstract  = {The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the last 7 years at major AI and ML conferences that introduce an XAI method. We find that 1 in 3 papers evaluate exclusively with anecdotal evidence, and 1 in 5 papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides researchers and practitioners with concrete tools to thoroughly validate, benchmark and compare new and existing XAI methods. The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as optimization criteria during model training in order to optimize for accuracy and interpretability simultaneously.},
  address   = {New York, NY, USA},
  doi       = {10.1145/3583558},
  keywords  = {explainability, explainable AI, explainable artificial intelligence, XAI, interpretable machine learning, interpretability, quantitative evaluation methods, evaluation},
  publisher = {Association for Computing Machinery},
  url       = {https://doi.org/10.1145/3583558},
}

@inproceedings{nautaInteractiveExplanationsInternal2020,
  title = {Interactive {{Explanations}} of {{Internal Representations}} of {{Neural Network Layers}}: {{An Exploratory Study}} on {{Outcome Prediction}} of {{Comatose Patients}}},
  booktitle = {International {{Workshop}} on {{Knowledge Discovery}} in {{Healthcare Data}}},
  author = {Nauta, Meike and van Putten, Michel J. A. M. and {Tjepkema-Cloostermans}, Marleen C. and Bos, Jeroen Peter and van Keulen, Maurice and Seifert, Christin},
  year = {2020},
  series = {{{KDH}}},
  volume = {2675}
}

@inproceedings{nautaIntrinsicallyInterpretableImage2021,
  title = {Intrinsically {{Interpretable Image Recognition}} with {{Neural Prototype Trees}}},
  booktitle = {Beyond {{Fairness}}: {{Towards}} a {{Just}}, {{Equitable}}, and {{Accountable Computer Vision}}: {{CVPR}} 2021 {{Workshop}}},
  author = {Nauta, Meike and van Bree, Ron and Seifert, Christin},
  year = {2021}
}

@inproceedings{nautaNeuralPrototypeTrees2021,
  title = {Neural {{Prototype Trees}} for {{Interpretable Fine-grained Image Recognition}}},
  booktitle = {{{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Nauta, Meike and van Bree, Ron and Seifert, Christin},
  year = {2021},
  pages = {14933--14943}
}

@inproceedings{nautaThisLooksThat2021,
  title = {This {{Looks Like That}}, {{Because}} ... {{Explaining Prototypes}} for {{Interpretable Image Recognition}}},
  booktitle = {Proc. {{ECML PKDD International Workshop}} on {{eXplainable Knowledge Discovery}} in {{Data Mining}} ({{XKDD}})},
  author = {Nauta, Meike and Jutte, Annemarie and Provoost, Jesper and Seifert, Christin},
  year = {2021}
}

@article{nautaUncoveringCorrectingShortcut2022,
  title = {Uncovering and {{Correcting Shortcut Learning}} in {{Machine Learning Models}} for {{Skin Cancer Diagnosis}}},
  author = {Nauta, Meike and Walsh, Ricky and Dubowski, Adam and Seifert, Christin},
  year = {2022},
  journal = {Diagnostics},
  volume = {12},
  number = {1},
  issn = {2075-4418},
  doi = {10.3390/diagnostics12010040},
  abstract = {Machine learning models have been successfully applied for analysis of skin images. However, due to the black box nature of such deep learning models, it is difficult to understand their underlying reasoning. This prevents a human from validating whether the model is right for the right reasons. Spurious correlations and other biases in data can cause a model to base its predictions on such artefacts rather than on the true relevant information. These learned shortcuts can in turn cause incorrect performance estimates and can result in unexpected outcomes when the model is applied in clinical practice. This study presents a method to detect and quantify this shortcut learning in trained classifiers for skin cancer diagnosis, since it is known that dermoscopy images can contain artefacts. Specifically, we train a standard VGG16-based skin cancer classifier on the public ISIC dataset, for which colour calibration charts (elliptical, coloured patches) occur only in benign images and not in malignant ones. Our methodology artificially inserts those patches and uses inpainting to automatically remove patches from images to assess the changes in predictions. We find that our standard classifier partly bases its predictions of benign images on the presence of such a coloured patch. More importantly, by artificially inserting coloured patches into malignant images, we show that shortcut learning results in a significant increase in misdiagnoses, making the classifier unreliable when used in clinical practice. With our results, we, therefore, want to increase awareness of the risks of using black box machine learning models trained on potentially biased datasets. Finally, we present a model-agnostic method to neutralise shortcut learning by removing the bias in the training dataset by exchanging coloured patches with benign skin tissue using image inpainting and re-training the classifier on this de-biased dataset.}
}

@inproceedings{Nguyen2022_iclr-blog_pplmrevisiteds-mammoth,
  title = {{{PPLM}} Revisited: {{Steering}} and Beaming a Lumbering Mammoth to Control Text Generation},
  booktitle = {{{ICLR}} Blog Track},
  author = {Nguyen, Van Bach and Trienes, Jan and Nauta, Meike and Pathak, Shreyasi and Youssef, Paul and Imangaliyev, Sultan and Schl{\"o}tterer, J{\"o}rg and Seifert, Christin},
  url = {https://iclr-blog-track.github.io/2022/03/25/PPLM/},
  year = {2022}
}

@misc{Nguyen2022_preprints_conversational-xai,
  title = {Explaining Machine Learning Models in Natural Conversations: {{Towards}} a Conversational {{XAI}} Agent},
  author = {Nguyen, Van Bach and Schl{\"o}tterer, J{\"o}rg and Seifert, Christin},
  year = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2209.02552},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences}
}

@inproceedings{nguyenHybridTextClassification2020,
  title = {A {{Hybrid Text Classification}} and {{Language Generation Model}} for {{Automated Summarization}} of {{Dutch Breast Cancer Radiology Reports}}},
  booktitle = {Proceedings {{International Conference}} on {{Cognitive Machine Intelligence}}},
  author = {Nguyen, Elisa and Theodorakopoulos, Daphne and Pathak, Shreyasik and Geerdink, Jeroen and Vijlbrief, Onno and van Keulen, Maurice and Seifert, Christin},
  year = {2020},
  series = {{{CogMI}}},
  publisher = {{IEEE}}
}

@inproceedings{papenmeierHowAccurateDoes2022,
  title = {How {{Accurate Does It Feel}}? \textendash{} {{Human Perception}} of {{Different Types}} of {{Classification Mistakes}}},
  booktitle = {Proceedings of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Papenmeier, Andrea and Kern, Dagmar and Hienert, Daniel and Kammerer, Yvonne and Seifert, Christin},
  year = {2022},
  series = {{{CHI}} '22},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3491102.3501915}
}

@article{papenmeierItComplicatedRelationship2022,
  title = {It's {{Complicated}}: {{The Relationship}} between {{User Trust}}, {{Model Accuracy}} and {{Explanations}} in {{AI}}},
  author = {Papenmeier, Andrea and Kern, Dagmar and Englebienne, Gwenn and Seifert, Christin},
  year = {2022},
  journal = {ACM Trans. Comput.-Hum. Interact.},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3495013}
}

@article{pathakSTQSInterpretableMultimodal2021,
  title = {{{STQS}}: {{Interpretable}} Multi-Modal {{Spatial-Temporal-seQuential}} Model for Automatic {{Sleep}} Scoring},
  author = {Pathak, Shreyasi and Lu, Changqing and Nagaraj, Sunil Belur and van Putten, Michel and Seifert, Christin},
  year = {2021},
  journal = {Artificial Intelligence in Medicine},
  volume = {114},
  pages = {102038},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2021.102038},
  keywords = {Deep learning,EEG,EMG signals,EOG,Explainable AI,Post-hoc interpretability,Sleep scoring,Sleep stage annotation},
  file = {/Users/andrea/Zotero/storage/ILEJPEMP/Pathak et al. - 2021 - STQS Interpretable multi-modal Spatial-Temporal-s.pdf}
}

@inproceedings{ruisHumanintheloopLanguageagnosticExtraction2020,
  title = {Human-in-the-Loop {{Language-agnostic Extraction}} of {{Medication Data}} from {{Highly Unstructured Electronic Health Records}}},
  booktitle = {Proc. {{International Conference}} on {{Data Mining Workshops}}},
  author = {Ruis, Frank and Pathak, Shreyasi and Geerdink, Jeroen and Hegeman, Johannes H. and Seifert, Christin and van Keulen, Maurice},
  year = {2020},
  publisher = {{IEEE}}
}

@article{schlottererQueryCrumbsSearchHistory2020,
  title = {{{QueryCrumbs Search History Visualization}} - {{Usability}}, {{Transparency}} and {{Long-term Usage}}},
  author = {Schl{\"o}tterer, J{\"o}rg and Seifert, Christin and Granitzer, Michael},
  year = {2020},
  journal = {Journal of Computer Languages},
  volume = {57},
  issn = {2590-1184},
  doi = {10.1016/j.cola.2020.100941},
  abstract = {Models of human information seeking reveal that search, in particular ad-hoc retrieval, is non-linear and iterative. Despite these findings, today's search user interfaces do not support non-linear navigation, like for example backtracking in time. We propose QueryCrumbs, a compact and easy-to-understand visualization for navigating the search query history supporting iterative query refinement. We apply a multi-layered interface design to support novices and first-time users as well as intermediate and expert users. The visualization is evaluated with novice users in a formative user study, with experts in a think aloud test and its usage in a long-term study with software logging. The formative evaluation showed that the interactions can be easily performed, and the visual encodings were well understood without instructions. Results indicate that QueryCrumbs can support users when searching for information in an iterative manner. The evaluation with experts showed that expert users can gain valuable insights into the back-end search engine by identifying specific patterns in the visualization. In a long-term usage study, we observed an uptake of the visualization, indicating that users deem QueryCrumbs beneficial for their search interactions.}
}

@inproceedings{schmidtTrustworthySecureReliable2021,
  title = {Towards a Trustworthy, Secure and Reliable Enclave for Machine Learning in a Hospital Setting: The {{Essen Medical Computing Platform}} ({{EMCP}})},
  booktitle = {Proc. {{International Conference}} on {{Cognitive Machine Intelligence}}},
  author = {Schmidt, Hendrik and Schl{\"o}tterer, J{\"o}rg and Bargull, Marcell and Nasca, Enrico and Aydelott, Ryan and Seifert, Christin and Meyer, Folker},
  year = {2021},
  publisher = {{IEEE}}
}

@inproceedings{Trienes2022_tsar_patient-friendly-clinical-notes,
  title = {Patient-Friendly Clinical Notes: {{Towards}} a New Text Simplification Dataset},
  booktitle = {Proc. {{EMNLP}} Workshop on Text Simplification, Accessibility, and Readability ({{TSAR}})},
  author = {Trienes, Jan and Schl{\"o}tterer, J{\"o}rg and Schildhaus, Hans-Ulrich and Seifert, Christin},
  year = {2022}
}

@inproceedings{trienesComparingRulebasedFeaturebased2020,
  title = {Comparing {{Rule-based}}, {{Feature-based}} and {{Deep Neural Methods}} for {{De-identification}} of {{Dutch Medical Records}}},
  booktitle = {Proc. {{Health Search}} and {{Data Mining Workshop}}},
  author = {Trienes, Jan and Trieschnigg, Dolf and Seifert, Christin and Hiemstra, Djoerd},
  year = {2020},
  series = {{{HSDM}}}
}

@article{vriesComparingThreeMachine2021,
  title = {Comparing Three Machine Learning Approaches to Design a Risk Assessment Tool for Future Fractures: Predicting a Subsequent Major Osteoporotic Fracture in Fracture Patients with Osteopenia and Osteoporosis},
  author = {de Vries, Bram C. S. and Hegeman, Johannes H. and Nijmeijer, Wieke and Geerdink, Jeroen and Seifert, Christin and {Groothuis-Oudshoorn}, Karin G. M.},
  year = {2021},
  month = jan,
  journal = {Osteoporosis International},
  issn = {1433-2965},
  doi = {10.1007/s00198-020-05735-z},
  abstract = {Four machine learning models were developed and compared to predict the risk of a future major osteoporotic fracture (MOF), defined as hip, wrist, spine and humerus fractures, in patients with a prior fracture. We developed a user-friendly tool for risk calculation of subsequent MOF in osteopenia patients, using the best performing model.}
}

@inproceedings{Youssef2022_dmbih_model-personalizaton-patient-data,
  title = {Model Personalization with Static and Dynamic Patients' Data},
  booktitle = {Proc. {{ICDM}} Workshop on Data Mining in Biomedical Informatics and Healthcare ({{DMBIH}})},
  author = {Youssef, Paul and Schl{\"o}tterer, J{\"o}rg and Imangaliyev, Sultan and Seifert, Christin},
  year = {2022}
}

@inproceedings{Zerhoudi2022_cikm_simiir20-framework,
  title = {The {{SimIIR}} 2.0 Framework: {{User}} Types, Markov Model-Based Interaction Simulation, and Advanced Query Generation},
  booktitle = {31st {{ACM}} International Conference on Information and Knowledge Management ({{CIKM}} 2022)},
  author = {Zerhoudi, Saber and G{\"u}nther, Sebastian and Plassmeier, Kim and Borst, Timo and Seifert, Christin and Hagen, Matthias and Granitzer, Michael},
  year = {2022},
  publisher = {{ACM}},
  site = {Atlanta, Georgia, USA}
}

@inproceedings{zerhoudiEvaluatingSimulatedUser2022,
  title = {Evaluating {{Simulated User Interaction}} and {{Search Behaviour}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Zerhoudi, Saber and Granitzer, Michael and Seifert, Christin and Schl{\"o}tterer, J{\"o}rg},
  year = {2022},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {DOI: 10.1007/978-3-030-99739-7_28}
}

@inproceedings{zerhoudiQueryChangeContextual2021,
  title = {Query {{Change}} as a {{Contextual Markov Model}} for {{Simulating User Search Behaviour}}},
  booktitle = {Proceedings of the 12th {{Forum}} for {{Information Retrieval Evaluation}}},
  author = {Zerhoudi, Saber and Granitzer, Michael and Schl{\"o}tterer, J{\"o}rg and Seifert, Christin},
  year = {2021},
  series = {{{FIRE}} '21},
  publisher = {{Association for Computing Machinery}},
  doi = {10.1145/3503162.3503165},
  abstract = {Search engine users issue queries to formulate their information need and gain useful insights. However, it is challenging for search engines to understand different users' search type intents and re- turn appropriate results. Simulating user search behaviour allows information retrieval systems (IR) to parameterise the a-priori dis- tribution of search types using different back-end configurations and user interface variants to improve the retrieval functionality. In this paper, we propose a formal Markov approach in which we utilise the context discovery process to model user-type specific behaviour by capturing the user's query change in a search session. Contextual Markov models have been used in the past to improve the prediction of user intentions, we investigate here their effi- ciency in simulating user-type specific interactions. Additionally, we provide an empirical and classification-based evaluation that can be used in simulation assessment. Overall, we report that the proposed approach reliably simulates user-type specific behaviour on a real-world academic search engine log dataset.}
}

@inproceedings{zerhoudiSimulatingUserInteraction2022,
  title = {Simulating {{User Interaction}} and {{Search Behaviour}} in {{Digital Libraries}}},
  booktitle = {Proc. {{Italian Research Conference}} on {{Digital Libraries}}},
  author = {Zerhoudi, Saber and Granitzer, Michael and Seifert, Christin and Schl{\"o}tterer, J{\"o}rg},
  year = {2022},
  series = {{{IRCDL}}}
}

